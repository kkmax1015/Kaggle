{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Cassava_EfficientNetB7_CutMix_and_MixUp_B16.ipynb","provenance":[{"file_id":"1YFT1z_sfyPcmb0vh0yOsmYc-LUqFRbtw","timestamp":1613060595188},{"file_id":"1Pb-3X1jgPs4j-LOj6L9T2xK9OFNgLQi1","timestamp":1612729631271}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":2705.650201,"end_time":"2021-01-20T23:42:08.938285","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-01-20T22:57:03.288084","version":"2.1.0"}},"cells":[{"cell_type":"code","metadata":{"id":"BkVGwREBxGQy","executionInfo":{"status":"ok","timestamp":1613071003331,"user_tz":-540,"elapsed":459,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["EPOCHS = 20 \n","HEIGHT = 512\n","WIDTH = 512\n","HEIGHT_RS = 512\n","WIDTH_RS = 512\n","CHANNELS = 3\n","N_CLASSES = 5\n","N_FOLDS = 5\n","FOLDS_USED = 5\n","ES_PATIENCE = 5\n","IMAGE_SIZE = [512, 512]"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZBauHrJCkn7","executionInfo":{"status":"ok","timestamp":1613071011648,"user_tz":-540,"elapsed":8766,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"71953376-3684-4558-af17-bac701c74bb6"},"source":["\n","import os\n","\n","models_path=''\n","\n","\n","\n","\n","\n","COLAB=True\n","import gc\n","!pip install fsspec\n","!pip install gcsfs \n","!pip install --upgrade --force-reinstall --no-deps kaggle\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/Colab Notebooks/Cassava/'\n","\n","#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n","\n","database_base_path = 'gs://kds-e118bcdb309cf88b7f9e4a96ee84997123a5781b886180ffc13d3fc9'\n","GCS_PATH = 'gs://kds-73ad24bb2e88a88d616d183727bf88d16deab7892840afc75a21cc90'\n","GCS_PATH_EXT = 'gs://kds-45c6060e9b2c39c0152e5fb66610110218c984e18c09a40d3f7b23d0'\n","GCS_PATH_CLASSES = 'gs://kds-214b31c2a1f93777a325b30beb67b72cda39e60b413aeaa16bff7396'\n","GCS_PATH_EXT_CLASSES = 'gs://kds-46c36b9b602c0da9228d8da71233604d6a4d98c9b36e7e9598d3dceb'\n","\n","\n","# \n","#This is a path to a dataset that changes over time, so you need to constantly update it. To update the path just run the code: \n","#GCS_DS_PATH = KaggleDatasets (). Get_gcs_path ()\n","#print (GCS_PATH)......\n","models_path='/content/drive/MyDrive/Colab Notebooks/Cassava/model/'# I created a folder called Models/Cassava on my Google Drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: fsspec in /usr/local/lib/python3.6/dist-packages (0.8.5)\n","Requirement already satisfied: gcsfs in /usr/local/lib/python3.6/dist-packages (0.7.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.6/dist-packages (from gcsfs) (3.7.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.23.0)\n","Requirement already satisfied: fsspec>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.8.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.2)\n","Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.25.0)\n","Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.4.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (5.1.0)\n","Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.7.4.3)\n","Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (1.1.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (20.3.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->gcsfs) (3.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.7)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (53.0.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.2->gcsfs) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.0)\n","Processing /root/.cache/pip/wheels/3a/d1/7e/6ce09b72b770149802c653a02783821629146983ee5a360f10/kaggle-1.5.10-cp36-none-any.whl\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.10\n","    Uninstalling kaggle-1.5.10:\n","      Successfully uninstalled kaggle-1.5.10\n","Successfully installed kaggle-1.5.10\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Cassava\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5V15UaoACh-M","executionInfo":{"status":"ok","timestamp":1613071011649,"user_tz":-540,"elapsed":8758,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["if COLAB:# Prepare the kaggle.json file for use \n","    from google.colab import files\n","    if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/.kaggle/kaggle.json'):\n","        !mkdir ~/content/drive/MyDrive/Colab Notebooks/.kaggle/\n","        if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/.kaggle/kaggle.json'):\n","            files.upload()\n","            !cp kaggle.json ~/content/drive/MyDrive/Colab Notebooks/.kaggle/\n","        else:\n","            !cp '/content/drive/MyDrive/Colab Notebooks/' ~/.kaggle/  \n","        !chmod 600 ~/content/drive/MyDrive/Colab Notebooks/.kaggle/kaggle.json\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"urYjsB4IV-t5","executionInfo":{"status":"ok","timestamp":1613071046651,"user_tz":-540,"elapsed":43753,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["if COLAB:# force TF to 2.2\n","    !pip install -q tensorflow~=2.2.0 tensorflow_gcs_config~=2.2.0\n","    \n","    import requests\n","    import os\n","    import tensorflow as tf\n","    resp = requests.post(\"http://{}:8475/requestversion/{}\".format(os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0], tf.__version__))\n","    if resp.status_code != 200:\n","      print(\"Failed to switch the TPU to TF {}\".format(version))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJLH967uCaOv"},"source":["## Dependencies"]},{"cell_type":"code","metadata":{"id":"u8lneueOCaOv","executionInfo":{"status":"ok","timestamp":1613071049588,"user_tz":-540,"elapsed":46681,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["!pip install --quiet efficientnet"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iMoIRz7CaOw","executionInfo":{"status":"ok","timestamp":1613071050794,"user_tz":-540,"elapsed":47880,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["import math, os, re, warnings, random, time\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import optimizers, Sequential, losses, metrics, Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","import efficientnet.tfkeras as efn\n","\n","def seed_everything(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","seed = 0\n","seed_everything(seed)\n","warnings.filterwarnings('ignore')"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-146lSGCaOx"},"source":["### Hardware configuration\n","\n","Note that we have `32` cores, this is because the `TPU v2 Pod` have more cores than a single `TPU v3` which has `8` cores."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cCFwEwkVCaOy","executionInfo":{"status":"ok","timestamp":1613071073360,"user_tz":-540,"elapsed":70438,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"0049fe83-0aa0-4fd4-b431-4de1031e394e"},"source":["# TPU or GPU detection\n","# Detect hardware, return appropriate distribution strategy\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print(f'Running on TPU {tpu.master()}')\n","except ValueError:\n","    tpu = None\n","    print ('tpu',tpu)\n","\n","if tpu:\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","else:\n","    strategy = tf.distribute.get_strategy()\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","REPLICAS = strategy.num_replicas_in_sync\n","print(f'REPLICAS: {REPLICAS}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Running on TPU grpc://10.110.41.250:8470\n","INFO:tensorflow:Initializing the TPU system: grpc://10.110.41.250:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.110.41.250:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["REPLICAS: 8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NK8TAx9JCaOz"},"source":["# Model parameters"]},{"cell_type":"code","metadata":{"id":"hyxJUKF-xGQ2","executionInfo":{"status":"ok","timestamp":1613071807482,"user_tz":-540,"elapsed":443,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["BATCH_SIZE = 16 * REPLICAS\n","AUG_BATCH = BATCH_SIZE\n","LEARNING_RATE = 1e-5 * REPLICAS"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UmwrOwlYCaOz"},"source":["# Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"UxM4MGNGCaO0","executionInfo":{"status":"ok","timestamp":1613071074690,"user_tz":-540,"elapsed":71755,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"5f5b702f-83d5-41a6-cb47-42cacdcc5b03"},"source":["def count_data_items(filenames):\n","    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n","    return np.sum(n)\n","\n","\n","train = pd.read_csv(f'{database_base_path}/train.csv')\n","print(f'Train samples: {len(train)}')\n","\n","FILENAMES_COMP = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')\n","FILENAMES_2019 = tf.io.gfile.glob(GCS_PATH_EXT + '/*.tfrec')\n","\n","FILENAMES_COMP_CBB = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CBB*.tfrec')\n","FILENAMES_COMP_CBSD = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CBSD*.tfrec')\n","FILENAMES_COMP_CGM = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CGM*.tfrec')\n","FILENAMES_COMP_CMD = tf.io.gfile.glob(GCS_PATH_CLASSES + '/CMD*.tfrec')\n","FILENAMES_COMP_Healthy = tf.io.gfile.glob(GCS_PATH_CLASSES + '/Healthy*.tfrec')\n","\n","FILENAMES_2019_CBB = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CBB*.tfrec')\n","FILENAMES_2019_CBSD = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CBSD*.tfrec')\n","FILENAMES_2019_CGM = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CGM*.tfrec')\n","FILENAMES_2019_CMD = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/CMD*.tfrec')\n","FILENAMES_2019_Healthy = tf.io.gfile.glob(GCS_PATH_EXT_CLASSES + '/Healthy*.tfrec')\n","\n","\n","TRAINING_FILENAMES = (FILENAMES_COMP + \n","                      FILENAMES_2019 + \n","                      (2 * FILENAMES_COMP_CBB) + \n","                      (2 * FILENAMES_2019_CBB) + \n","                      (2 * FILENAMES_COMP_CBSD) + \n","                      (2 * FILENAMES_2019_CBSD) + \n","                      (2 * FILENAMES_COMP_CGM) + \n","                      (2 * FILENAMES_2019_CGM) + \n","                      (2 * FILENAMES_COMP_Healthy) + \n","                      (2 * FILENAMES_2019_Healthy))\n","\n","NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n","\n","print(f'GCS: train images: {NUM_TRAINING_IMAGES}')\n","display(train.head())\n","\n","CLASSES = ['Cassava Bacterial Blight', \n","           'Cassava Brown Streak Disease', \n","           'Cassava Green Mottle', \n","           'Cassava Mosaic Disease', \n","           'Healthy']"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable onattempt 1 of 3. Reason: [Errno 115] Operation now in progress\n","WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable onattempt 2 of 3. Reason: [Errno 115] Operation now in progress\n","WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable onattempt 3 of 3. Reason: [Errno 115] Operation now in progress\n","WARNING:google.auth._default:Authentication failed using Compute Engine authentication due to unavailable metadata server.\n"],"name":"stderr"},{"output_type":"stream","text":["Train samples: 21397\n","GCS: train images: 48081\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000015157.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000201771.jpg</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100042118.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000723321.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000812911.jpg</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         image_id  label\n","0  1000015157.jpg      0\n","1  1000201771.jpg      3\n","2   100042118.jpg      1\n","3  1000723321.jpg      1\n","4  1000812911.jpg      3"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"BZIhYsg_CaO1"},"source":["# Augmentation"]},{"cell_type":"code","metadata":{"id":"Wwh329L6CaO1","executionInfo":{"status":"ok","timestamp":1613071074692,"user_tz":-540,"elapsed":71749,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["def data_augment(image, label):\n","    p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    \n","    # Shear\n","    if p_shear > .2:\n","        if p_shear > .6:\n","            image = transform_shear(image, HEIGHT, shear=20.)\n","        else:\n","            image = transform_shear(image, HEIGHT, shear=-20.)\n","            \n","    # Rotation\n","    if p_rotation > .2:\n","        if p_rotation > .6:\n","            image = transform_rotation(image, HEIGHT, rotation=45.)\n","        else:\n","            image = transform_rotation(image, HEIGHT, rotation=-45.)\n","            \n","    # Flips\n","    image = tf.image.random_flip_left_right(image)\n","    image = tf.image.random_flip_up_down(image)\n","    if p_spatial > .75:\n","        image = tf.image.transpose(image)\n","        \n","    # Rotates\n","    if p_rotate > .75:\n","        image = tf.image.rot90(image, k=3) # rotate 270º\n","    elif p_rotate > .5:\n","        image = tf.image.rot90(image, k=2) # rotate 180º\n","    elif p_rotate > .25:\n","        image = tf.image.rot90(image, k=1) # rotate 90º\n","        \n","    # Pixel-level transforms\n","    if p_pixel_1 >= .4:\n","        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n","    if p_pixel_2 >= .4:\n","        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n","    if p_pixel_3 >= .4:\n","        image = tf.image.random_brightness(image, max_delta=.1)\n","        \n","    # Crops\n","    if p_crop > .6:\n","        if p_crop > .9:\n","            image = tf.image.central_crop(image, central_fraction=.5)\n","        elif p_crop > .8:\n","            image = tf.image.central_crop(image, central_fraction=.6)\n","        elif p_crop > .7:\n","            image = tf.image.central_crop(image, central_fraction=.7)\n","        else:\n","            image = tf.image.central_crop(image, central_fraction=.8)\n","    elif p_crop > .3:\n","        crop_size = tf.random.uniform([], int(HEIGHT*.6), HEIGHT, dtype=tf.int32)\n","        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n","            \n","    image = tf.image.resize(image, size=[HEIGHT, WIDTH])\n","\n","    # if p_cutout > .5:\n","    #     image = data_augment_cutout(image)\n","        \n","    return image, label"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4Hg1nKgCaO1"},"source":["## Auxiliary functions"]},{"cell_type":"code","metadata":{"id":"-DueNRgfCaO1","executionInfo":{"status":"ok","timestamp":1613071074876,"user_tz":-540,"elapsed":71927,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["# data augmentation @cdeotte kernel: https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96\n","def transform_rotation(image, height, rotation):\n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    # output - image randomly rotated\n","    DIM = height\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    rotation = rotation * tf.random.uniform([1],dtype='float32')\n","    # CONVERT DEGREES TO RADIANS\n","    rotation = math.pi * rotation / 180.\n","    \n","    # ROTATION MATRIX\n","    c1 = tf.math.cos(rotation)\n","    s1 = tf.math.sin(rotation)\n","    one = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    rotation_matrix = tf.reshape(tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3])\n","\n","    # LIST DESTINATION PIXEL INDICES\n","    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n","    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n","    z = tf.ones([DIM*DIM],dtype='int32')\n","    idx = tf.stack( [x,y,z] )\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(rotation_matrix,tf.cast(idx,dtype='float32'))\n","    idx2 = K.cast(idx2,dtype='int32')\n","    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES \n","    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n","    d = tf.gather_nd(image, tf.transpose(idx3))\n","        \n","    return tf.reshape(d,[DIM,DIM,3])\n","\n","def transform_shear(image, height, shear):\n","    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n","    # output - image randomly sheared\n","    DIM = height\n","    XDIM = DIM%2 #fix for size 331\n","    \n","    shear = shear * tf.random.uniform([1],dtype='float32')\n","    shear = math.pi * shear / 180.\n","        \n","    # SHEAR MATRIX\n","    one = tf.constant([1],dtype='float32')\n","    zero = tf.constant([0],dtype='float32')\n","    c2 = tf.math.cos(shear)\n","    s2 = tf.math.sin(shear)\n","    shear_matrix = tf.reshape(tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3])    \n","\n","    # LIST DESTINATION PIXEL INDICES\n","    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n","    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n","    z = tf.ones([DIM*DIM],dtype='int32')\n","    idx = tf.stack( [x,y,z] )\n","    \n","    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n","    idx2 = K.dot(shear_matrix,tf.cast(idx,dtype='float32'))\n","    idx2 = K.cast(idx2,dtype='int32')\n","    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n","    \n","    # FIND ORIGIN PIXEL VALUES \n","    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n","    d = tf.gather_nd(image, tf.transpose(idx3))\n","        \n","    return tf.reshape(d,[DIM,DIM,3])\n","\n","# CutOut\n","def data_augment_cutout(image, min_mask_size=(int(HEIGHT * .1), int(HEIGHT * .1)), \n","                        max_mask_size=(int(HEIGHT * .125), int(HEIGHT * .125))):\n","    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    \n","    if p_cutout > .85: # 10~15 cut outs\n","        n_cutout = tf.random.uniform([], 10, 15, dtype=tf.int32)\n","        image = random_cutout(image, HEIGHT, WIDTH, \n","                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n","    elif p_cutout > .6: # 5~10 cut outs\n","        n_cutout = tf.random.uniform([], 5, 10, dtype=tf.int32)\n","        image = random_cutout(image, HEIGHT, WIDTH, \n","                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n","    elif p_cutout > .25: # 2~5 cut outs\n","        n_cutout = tf.random.uniform([], 2, 5, dtype=tf.int32)\n","        image = random_cutout(image, HEIGHT, WIDTH, \n","                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=n_cutout)\n","    else: # 1 cut out\n","        image = random_cutout(image, HEIGHT, WIDTH, \n","                              min_mask_size=min_mask_size, max_mask_size=max_mask_size, k=1)\n","\n","    return image\n","\n","def random_cutout(image, height, width, channels=3, min_mask_size=(10, 10), max_mask_size=(80, 80), k=1):\n","    assert height > min_mask_size[0]\n","    assert width > min_mask_size[1]\n","    assert height > max_mask_size[0]\n","    assert width > max_mask_size[1]\n","\n","    for i in range(k):\n","      mask_height = tf.random.uniform(shape=[], minval=min_mask_size[0], maxval=max_mask_size[0], dtype=tf.int32)\n","      mask_width = tf.random.uniform(shape=[], minval=min_mask_size[1], maxval=max_mask_size[1], dtype=tf.int32)\n","\n","      pad_h = height - mask_height\n","      pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n","      pad_bottom = pad_h - pad_top\n","\n","      pad_w = width - mask_width\n","      pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n","      pad_right = pad_w - pad_left\n","\n","      cutout_area = tf.zeros(shape=[mask_height, mask_width, channels], dtype=tf.uint8)\n","\n","      cutout_mask = tf.pad([cutout_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n","      cutout_mask = tf.squeeze(cutout_mask, axis=0)\n","      image = tf.multiply(tf.cast(image, tf.float32), tf.cast(cutout_mask, tf.float32))\n","\n","    return image"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyi3J0MXvJBz","executionInfo":{"status":"ok","timestamp":1613071074877,"user_tz":-540,"elapsed":71922,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["def cutmix(image, label, PROBABILITY = 1.0):\r\n","    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\r\n","    # output - a batch of images with cutmix applied\r\n","    DIM = IMAGE_SIZE[0]\r\n","    CLASSES = N_CLASSES\r\n","    \r\n","    imgs = []; labs = []\r\n","    for j in range(AUG_BATCH):\r\n","        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\r\n","        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\r\n","        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\r\n","        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\r\n","        # CHOOSE RANDOM LOCATION\r\n","        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\r\n","        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\r\n","        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\r\n","        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\r\n","        ya = tf.math.maximum(0,y-WIDTH//2)\r\n","        yb = tf.math.minimum(DIM,y+WIDTH//2)\r\n","        xa = tf.math.maximum(0,x-WIDTH//2)\r\n","        xb = tf.math.minimum(DIM,x+WIDTH//2)\r\n","        # MAKE CUTMIX IMAGE\r\n","        one = image[j,ya:yb,0:xa,:]\r\n","        two = image[k,ya:yb,xa:xb,:]\r\n","        three = image[j,ya:yb,xb:DIM,:]\r\n","        middle = tf.concat([one,two,three],axis=1)\r\n","        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\r\n","        imgs.append(img)\r\n","        # MAKE CUTMIX LABEL\r\n","        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\r\n","        if len(label.shape)==1:\r\n","            lab1 = tf.one_hot(label[j],CLASSES)\r\n","            lab2 = tf.one_hot(label[k],CLASSES)\r\n","        else:\r\n","            lab1 = label[j,]\r\n","            lab2 = label[k,]\r\n","        labs.append((1-a)*lab1 + a*lab2)\r\n","            \r\n","    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\r\n","    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\r\n","    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\r\n","    return image2,label2"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"VD277mUEvLBE","executionInfo":{"status":"ok","timestamp":1613071075209,"user_tz":-540,"elapsed":72249,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["def mixup(image, label, PROBABILITY = 1.0):\r\n","    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\r\n","    # output - a batch of images with mixup applied\r\n","    DIM = IMAGE_SIZE[0]\r\n","    CLASSES = N_CLASSES\r\n","    \r\n","    imgs = []; labs = []\r\n","    for j in range(AUG_BATCH):\r\n","        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\r\n","        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\r\n","        # CHOOSE RANDOM\r\n","        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\r\n","        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\r\n","        # MAKE MIXUP IMAGE\r\n","        img1 = image[j,]\r\n","        img2 = image[k,]\r\n","        imgs.append((1-a)*img1 + a*img2)\r\n","        # MAKE CUTMIX LABEL\r\n","        if len(label.shape)==1:\r\n","            lab1 = tf.one_hot(label[j],CLASSES)\r\n","            lab2 = tf.one_hot(label[k],CLASSES)\r\n","        else:\r\n","            lab1 = label[j,]\r\n","            lab2 = label[k,]\r\n","        labs.append((1-a)*lab1 + a*lab2)\r\n","            \r\n","    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\r\n","    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\r\n","    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\r\n","    return image2,label2"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdQEC_iKvR3z","executionInfo":{"status":"ok","timestamp":1613071075209,"user_tz":-540,"elapsed":72244,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["def transform(image,label):\r\n","    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\r\n","    DIM = IMAGE_SIZE[0]\r\n","    CLASSES = N_CLASSES\r\n","    SWITCH = 0.5\r\n","    CUTMIX_PROB = 0.666\r\n","    MIXUP_PROB = 0.666\r\n","    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\r\n","    image2, label2 = cutmix(image, label, CUTMIX_PROB)\r\n","    image3, label3 = mixup(image, label, MIXUP_PROB)\r\n","    imgs = []; labs = []\r\n","    for j in range(AUG_BATCH):\r\n","        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\r\n","        imgs.append(P*image2[j,]+(1-P)*image3[j,])\r\n","        labs.append(P*label2[j,]+(1-P)*label3[j,])\r\n","    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\r\n","    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\r\n","    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\r\n","    return image4,label4"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPcg0bWECaO1","executionInfo":{"status":"ok","timestamp":1613071075210,"user_tz":-540,"elapsed":72237,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["# Datasets utility functions\n","def decode_image(image_data):\n","    \"\"\"\n","        Decode a JPEG-encoded image to a uint8 tensor.\n","    \"\"\"\n","    image = tf.image.decode_jpeg(image_data, channels=3)\n","    return image\n","\n","def scale_image(image, label):\n","    \"\"\"\n","        Cast tensor to float and normalizes (range between 0 and 1).\n","    \"\"\"\n","    image = tf.cast(image, tf.float32)\n","    image /= 255.0\n","    return image, label\n","\n","def prepare_image(image, label):\n","    \"\"\"\n","        Resize and reshape images to the expected size.\n","    \"\"\"\n","    image = tf.image.resize(image, [HEIGHT_RS, WIDTH_RS])\n","    image = tf.reshape(image, [HEIGHT_RS, WIDTH_RS, 3])\n","    return image, label\n","\n","def read_tfrecord(example, labeled=True):\n","    \"\"\"\n","        1. Parse data based on the 'TFREC_FORMAT' map.\n","        2. Decode image.\n","        3. If 'labeled' returns (image, label) if not (image, name).\n","    \"\"\"\n","    if labeled:\n","        TFREC_FORMAT = {\n","            'image': tf.io.FixedLenFeature([], tf.string), \n","            'target': tf.io.FixedLenFeature([], tf.int64), \n","        }\n","    else:\n","        TFREC_FORMAT = {\n","            'image': tf.io.FixedLenFeature([], tf.string), \n","            'image_name': tf.io.FixedLenFeature([], tf.string), \n","        }\n","    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n","    image = decode_image(example['image'])\n","    if labeled:\n","        label_or_name = tf.cast(example['target'], tf.int32)\n","        # One-Hot Encoding needed to use \"categorical_crossentropy\" loss\n","        label_or_name = tf.one_hot(tf.cast(label_or_name, tf.int32), N_CLASSES)\n","    else:\n","        label_or_name = example['image_name']\n","    return image, label_or_name\n","\n","def get_dataset(FILENAMES, labeled=True, ordered=False, repeated=False, \n","                cached=False, augment=False):\n","    \"\"\"\n","        Return a Tensorflow dataset ready for training or inference.\n","    \"\"\"\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False\n","        dataset = tf.data.Dataset.list_files(FILENAMES)\n","        dataset = dataset.interleave(tf.data.TFRecordDataset, num_parallel_calls=AUTO)\n","    else:\n","        dataset = tf.data.TFRecordDataset(FILENAMES, num_parallel_reads=AUTO)\n","        \n","    dataset = dataset.with_options(ignore_order)\n","    \n","    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTO)\n","    \n","    if augment:\n","        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n","\n","    if repeated:\n","        dataset = dataset.repeat()\n","        \n","    dataset = dataset.batch(AUG_BATCH)\n","    if augment: \n","        dataset = dataset.map(transform, num_parallel_calls=AUTO) # note we put AFTER batching\n","    dataset = dataset.unbatch()\n","        \n","    dataset = dataset.map(scale_image, num_parallel_calls=AUTO)\n","    dataset = dataset.map(prepare_image, num_parallel_calls=AUTO)\n","    \n","    if not ordered:\n","        dataset = dataset.shuffle(2048)\n","    # if repeated:\n","    #     dataset = dataset.repeat()\n","        \n","    dataset = dataset.batch(BATCH_SIZE)\n","    \n","    if cached:\n","        dataset = dataset.cache()\n","    dataset = dataset.prefetch(AUTO)\n","\n","    return dataset\n","\n","def unfreeze_model(model):\n","    # Unfreeze layers while leaving BatchNorm layers frozen\n","    for layer in model.layers:\n","        if not isinstance(layer, L.BatchNormalization):\n","            layer.trainable = True\n","        else:\n","            layer.trainable = False\n","                \n","def unfreeze_block(model, block_name=None, n_top=3):\n","    # Unfreeze layers while leaving BatchNorm layers frozen\n","    for layer in model.layers[:-n_top]:\n","        if isinstance(layer, L.BatchNormalization):\n","            layer.trainable = False\n","        else:\n","            if block_name and (block_name in layer.name):\n","                layer.trainable = True"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHAxNOInCaO2","executionInfo":{"status":"ok","timestamp":1613071075390,"user_tz":-540,"elapsed":72412,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["# Visualization utility functions\n","np.set_printoptions(threshold=15, linewidth=80)\n","\n","def batch_to_numpy_images_and_labels(data):\n","    images, labels = data\n","    numpy_images = images.numpy()\n","    numpy_labels = labels.numpy()\n","    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n","        numpy_labels = [None for _ in enumerate(numpy_images)]\n","    # If no labels, only image IDs, return None for labels (this is the case for test data)\n","    return numpy_images, numpy_labels\n","\n","def title_from_label_and_target(label, correct_label):\n","    if correct_label is None:\n","        return CLASSES[label], True\n","    correct = (label == correct_label)\n","    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n","                                CLASSES[correct_label] if not correct else ''), correct\n","\n","def display_one_flower(image, title, subplot, red=False, titlesize=16):\n","    plt.subplot(*subplot)\n","    plt.axis('off')\n","    plt.imshow(image)\n","    if len(title) > 0:\n","        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', \n","                  fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n","    return (subplot[0], subplot[1], subplot[2]+1)\n","\n","def display_batch_of_images(databatch, predictions=None):\n","    \"\"\"This will work with:\n","    display_batch_of_images(images)\n","    display_batch_of_images(images, predictions)\n","    display_batch_of_images((images, labels))\n","    display_batch_of_images((images, labels), predictions)\n","    \"\"\"\n","    # data\n","    images, labels = batch_to_numpy_images_and_labels(databatch)\n","    labels = np.argmax(labels, axis=-1)\n","    if labels is None:\n","        labels = [None for _ in enumerate(images)]\n","        \n","    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n","    rows = int(math.sqrt(len(images)))\n","    cols = len(images)//rows\n","        \n","    # size and spacing\n","    FIGSIZE = 13.0\n","    SPACING = 0.1\n","    subplot=(rows,cols,1)\n","    if rows < cols:\n","        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n","    else:\n","        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n","    \n","    # display\n","    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n","        title = '' if label is None else CLASSES[label]\n","        correct = True\n","        if predictions is not None:\n","            title, correct = title_from_label_and_target(predictions[i], label)\n","        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n","        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n","    \n","    #layout\n","    plt.tight_layout()\n","    if label is None and predictions is None:\n","        plt.subplots_adjust(wspace=0, hspace=0)\n","    else:\n","        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n","    plt.show()\n","    \n","# Visualize model predictions\n","def dataset_to_numpy_util(dataset, N):\n","    dataset = dataset.unbatch().batch(N)\n","    for images, labels in dataset:\n","        numpy_images = images.numpy()\n","        numpy_labels = labels.numpy()\n","        break;  \n","    return numpy_images, numpy_labels\n","\n","def title_from_label_and_target(label, correct_label):\n","    label = np.argmax(label, axis=-1)\n","    correct = (label == correct_label)\n","    return \"{} [{}{}{}]\".format(label, str(correct), ', shoud be ' if not correct else '',\n","                                correct_label if not correct else ''), correct\n","\n","def display_one_flower_eval(image, title, subplot, red=False):\n","    plt.subplot(subplot)\n","    plt.axis('off')\n","    plt.imshow(image)\n","    plt.title(title, fontsize=14, color='red' if red else 'black')\n","    return subplot+1\n","\n","def display_9_images_with_predictions(images, predictions, labels):\n","    subplot=331\n","    plt.figure(figsize=(13,13))\n","    for i, image in enumerate(images):\n","        title, correct = title_from_label_and_target(predictions[i], labels[i])\n","        subplot = display_one_flower_eval(image, title, subplot, not correct)\n","        if i >= 8:\n","            break;\n","              \n","    plt.tight_layout()\n","    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","    plt.show()\n","\n","\n","# Model evaluation\n","def plot_metrics(history):\n","    fig, axes = plt.subplots(2, 1, sharex='col', figsize=(20, 8))\n","    axes = axes.flatten()\n","    \n","    axes[0].plot(history['loss'], label='Train loss')\n","    axes[0].plot(history['val_loss'], label='Validation loss')\n","    axes[0].legend(loc='best', fontsize=16)\n","    axes[0].set_title('Loss')\n","    axes[0].axvline(np.argmin(history['loss']), linestyle='dashed')\n","    axes[0].axvline(np.argmin(history['val_loss']), linestyle='dashed', color='orange')\n","    \n","    axes[1].plot(history['accuracy'], label='Train accuracy')\n","    axes[1].plot(history['val_accuracy'], label='Validation accuracy')\n","    axes[1].legend(loc='best', fontsize=16)\n","    axes[1].set_title('Accuracy')\n","    axes[1].axvline(np.argmax(history['accuracy']), linestyle='dashed')\n","    axes[1].axvline(np.argmax(history['val_accuracy']), linestyle='dashed', color='orange')\n","\n","    plt.xlabel('Epochs', fontsize=16)\n","    sns.despine()\n","    plt.show()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oCtCB3WdCaO2"},"source":["# Training data samples (with augmentation)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17YaJ1G0dowe-OlZX3bmPVdhwxTOSSaWc"},"id":"p8IMzXLRCaO2","executionInfo":{"status":"ok","timestamp":1613071138926,"user_tz":-540,"elapsed":135942,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"c2158399-8360-49cb-8fcd-041b02dfb504"},"source":["train_dataset = get_dataset(FILENAMES_COMP, ordered=True, augment=True)\n","train_iter = iter(train_dataset.unbatch().batch(20))\n","\n","display_batch_of_images(next(train_iter))\n","display_batch_of_images(next(train_iter))"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"HvXRzhofCaO5"},"source":["### Learning rate schedule\n","\n","We are going to use a `cosine learning rate schedule with a warm-up phase`, this may be a good idea since we are using a pre-trained model, the warm-up phase will be useful to avoid the pre-trained weights degradation resulting in catastrophic forgetting, during the schedule the learning rate will slowly decrease to very low values, this helps the model to land on more stable weights."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422},"id":"I9cuJCl1CaO5","executionInfo":{"status":"ok","timestamp":1613071182647,"user_tz":-540,"elapsed":179655,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"b58f7f2a-9f4c-42b6-d110-3e3f8d2dd494"},"source":["lr_start = 1e-8\n","lr_min = 1e-8\n","lr_max = LEARNING_RATE\n","num_cycles = 1.\n","warmup_epochs = 1\n","hold_max_epochs = 0\n","total_epochs = EPOCHS\n","warmup_steps = warmup_epochs * (NUM_TRAINING_IMAGES//BATCH_SIZE)\n","total_steps = total_epochs * (NUM_TRAINING_IMAGES//BATCH_SIZE)\n","\n","@tf.function\n","def lrfn(step):\n","    if step < warmup_steps:\n","        lr = (lr_max - lr_start) / warmup_steps * step + lr_start\n","    else:\n","        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n","        lr = lr_max * (0.5 * (1.0 + tf.math.cos(np.pi * ((num_cycles * progress) % 1.0))))\n","        if lr_min is not None:\n","            lr = tf.math.maximum(lr_min, float(lr))\n","\n","    return lr\n","\n","\n","# rng = [i for i in range(total_epochs)]\n","rng = [i for i in range(total_steps)]\n","y = [lrfn(tf.cast(x, tf.float32)) for x in rng]\n","\n","sns.set(style='whitegrid')\n","fig, ax = plt.subplots(figsize=(20, 6))\n","plt.plot(rng, y)\n","\n","print(f'{total_steps} total steps and {NUM_TRAINING_IMAGES//BATCH_SIZE} steps per epoch')\n","print(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["3740 total steps and 187 steps per epoch\n","Learning rate schedule: 1e-08 to 8e-05 to 1e-08\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1440x432 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"OMIrA5jxCaO5"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"pRksD6K6CaO5","executionInfo":{"status":"ok","timestamp":1613071182648,"user_tz":-540,"elapsed":179647,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["def model_fn(input_shape, N_CLASSES):\n","    inputs = L.Input(shape=input_shape, name='input_image')\n","    base_model = efn.EfficientNetB7(input_tensor=inputs, \n","                                    include_top=False, \n","                                    weights='noisy-student', \n","                                    pooling='avg')\n","    base_model.trainable = False\n","    x = L.Dropout(.5)(base_model.output)\n","    output = L.Dense(N_CLASSES, activation='softmax', name='output')(x)\n","    model = Model(inputs=inputs, outputs=output)\n","\n","    return model"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"4zl0qy-pcR-A","executionInfo":{"status":"ok","timestamp":1613071182650,"user_tz":-540,"elapsed":179644,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["#help(strategy)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hMc0tk0VCaO5"},"source":["# Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"8HrFGFRNCaO5","executionInfo":{"status":"error","timestamp":1613072590387,"user_tz":-540,"elapsed":775265,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}},"outputId":"e394c005-d998-44f8-c595-3adb585df5da"},"source":["\n","skf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n","oof_pred = []; oof_labels = []; history_list = []\n","\n","for fold,(idxT, idxV) in enumerate(skf.split(np.arange(50))):\n","    if fold >= FOLDS_USED:\n","        break\n","    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n","    K.clear_session()\n","    print(f'\\nFOLD: {fold+1}')\n","    print(f'TRAIN: {idxT} VALID: {idxV}')\n","\n","    # Create train and validation sets\n","    FILENAMES_COMP = tf.io.gfile.glob([GCS_PATH + '/Id_train%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_2019 = tf.io.gfile.glob([GCS_PATH_EXT + '/Id_train%.2i*.tfrec' % x for x in idxT])\n","\n","    FILENAMES_COMP_CBB = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CBB%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_COMP_CBSD = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CBSD%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_COMP_CGM = tf.io.gfile.glob([GCS_PATH_CLASSES + '/CGM%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_COMP_Healthy = tf.io.gfile.glob([GCS_PATH_CLASSES + '/Healthy%.2i*.tfrec' % x for x in idxT])\n","    \n","    FILENAMES_2019_CBB = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CBB%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_2019_CBSD = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CBSD%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_2019_CGM = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/CGM%.2i*.tfrec' % x for x in idxT])\n","    FILENAMES_2019_Healthy = tf.io.gfile.glob([GCS_PATH_EXT_CLASSES + '/Healthy%.2i*.tfrec' % x for x in idxT])\n","\n","    TRAIN_FILENAMES = (FILENAMES_COMP + \n","                       FILENAMES_2019 + \n","                       (2 * FILENAMES_COMP_CBB) + \n","                       (2 * FILENAMES_2019_CBB) + \n","                       (2 * FILENAMES_COMP_CBSD) + \n","                       (2 * FILENAMES_2019_CBSD) + \n","                       (2 * FILENAMES_COMP_CGM) + \n","                       (2 * FILENAMES_2019_CGM) + \n","                       (2 * FILENAMES_COMP_Healthy) + \n","                       (2 * FILENAMES_2019_Healthy))\n","    \n","    VALID_FILENAMES = tf.io.gfile.glob([GCS_PATH + '/Id_train%.2i*.tfrec' % x for x in idxV])\n","    np.random.shuffle(TRAIN_FILENAMES)\n","    \n","    ct_train = count_data_items(TRAIN_FILENAMES)\n","    ct_valid = count_data_items(VALID_FILENAMES)\n","    \n","    step_size = (ct_train // BATCH_SIZE)\n","    valid_step_size = (ct_valid // BATCH_SIZE)\n","    total_steps=(total_epochs * step_size)\n","    warmup_steps=(warmup_epochs * step_size)\n","    \n","    \n","    # Build TF datasets\n","    train_ds = strategy.experimental_distribute_dataset(get_dataset(TRAIN_FILENAMES, repeated=True, augment=True))\n","    valid_ds = strategy.experimental_distribute_dataset(get_dataset(VALID_FILENAMES, ordered=True, repeated=True, cached=True))\n","    train_data_iter = iter(train_ds)\n","    valid_data_iter = iter(valid_ds)\n","    \n","    \n","    # Step functions\n","    @tf.function\n","    def train_step(data_iter):\n","        def train_step_fn(x, y):\n","            with tf.GradientTape() as tape:\n","                probabilities = model(x, training=True)\n","                loss = loss_fn(y, probabilities, label_smoothing=.3)\n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            # update metrics\n","            train_accuracy.update_state(y, probabilities)\n","            train_loss.update_state(loss)\n","        for _ in tf.range(step_size):\n","            if COLAB:\n","                #print(COLAB)\n","                strategy.experimental_run_v2(train_step_fn, next(data_iter))\n","            else:\n","                strategy.experimental_run_v2(train_step_fn, next(data_iter))\n","\n","    @tf.function\n","    def valid_step(data_iter):\n","        def valid_step_fn(x, y):\n","            probabilities = model(x, training=False)\n","            loss = loss_fn(y, probabilities)\n","            # update metrics\n","            valid_accuracy.update_state(y, probabilities)\n","            valid_loss.update_state(loss)\n","        for _ in tf.range(valid_step_size):\n","            if COLAB:\n","                strategy.experimental_run_v2(valid_step_fn, next(data_iter))\n","            else:\n","                strategy.experimental_run_v2(valid_step_fn, next(data_iter))\n","    \n","    \n","    # Model\n","    model_path = models_path+f'model_{fold}.h5'\n","    with strategy.scope():\n","        model = model_fn((None, None, CHANNELS), N_CLASSES)\n","        unfreeze_model(model) # unfreeze all layers except \"batch normalization\"\n","        \n","        optimizer = optimizers.Adam(learning_rate=lambda: lrfn(tf.cast(optimizer.iterations, tf.float32)))\n","        loss_fn = losses.categorical_crossentropy\n","\n","        train_accuracy = metrics.CategoricalAccuracy()\n","        valid_accuracy = metrics.CategoricalAccuracy()\n","        train_loss = metrics.Sum()\n","        valid_loss = metrics.Sum()\n","    \n","    \n","    # Setup training loop\n","    step = 0\n","    epoch_steps = 0\n","    patience_cnt = 0\n","    best_val = 0\n","    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n","\n","    ### Train model\n","    for epoch in range(EPOCHS):\n","        epoch_start_time = time.time()\n","\n","        # Run training step\n","        train_step(train_data_iter)\n","        epoch_steps += step_size\n","        step += step_size\n","            \n","\n","        # Validation run at the end of each epoch\n","        if (step // step_size) > epoch:\n","            # Validation run\n","            valid_epoch_steps = 0\n","            valid_step(valid_data_iter)\n","            valid_epoch_steps += valid_step_size\n","\n","            # Compute metrics\n","            history['accuracy'].append(train_accuracy.result().numpy())\n","            history['loss'].append(train_loss.result().numpy() / (BATCH_SIZE * epoch_steps))\n","            history['val_accuracy'].append(valid_accuracy.result().numpy())\n","            history['val_loss'].append(valid_loss.result().numpy() / (BATCH_SIZE * valid_epoch_steps))\n","\n","            # Report metrics\n","            epoch_time = time.time() - epoch_start_time\n","            print(f'\\nEPOCH {epoch+1}/{EPOCHS}')\n","            print(f'time: {epoch_time:0.1f}s',\n","                  f\"loss: {history['loss'][-1]:0.4f}\",\n","                  f\"accuracy: {history['accuracy'][-1]:0.4f}\",\n","                  f\"val_loss: {history['val_loss'][-1]:0.4f}\",\n","                  f\"val_accuracy: {history['val_accuracy'][-1]:0.4f}\",\n","                  f'lr: {lrfn(tf.cast(optimizer.iterations, tf.int32).numpy()):0.4g}')\n","\n","            # Early stopping monitor\n","            if history['val_accuracy'][-1] >= best_val:\n","                best_val = history['val_accuracy'][-1]\n","                model.save_weights(model_path)\n","                print(f'Saved model weights at \"{model_path}\"')\n","                patience_cnt = 1\n","            else:\n","                patience_cnt += 1\n","            # if patience_cnt > ES_PATIENCE:\n","            #     print(f'Epoch {epoch:05d}: early stopping')\n","            #     break\n","\n","                \n","            # Set up next epoch\n","            epoch = step // step_size\n","            epoch_steps = 0\n","            train_accuracy.reset_states()\n","            train_loss.reset_states()\n","            valid_accuracy.reset_states()\n","            valid_loss.reset_states()\n","    \n","    \n","    ### RESULTS\n","    print(f\"#### FOLD {fold+1} OOF Accuracy = {np.max(history['val_accuracy']):.3f}\")\n","    \n","    history_list.append(history)\n","    # Load best model weights\n","    model.load_weights(model_path)\n","\n","    # OOF predictions\n","    ds_valid = get_dataset(VALID_FILENAMES, ordered=True)\n","    oof_labels.append([target.numpy() for img, target in iter(ds_valid.unbatch())])\n","    x_oof = ds_valid.map(lambda image, target: image)\n","    oof_pred.append(np.argmax(model.predict(x_oof), axis=-1))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:TPU system grpc://10.110.41.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:TPU system grpc://10.110.41.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.110.41.250:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.110.41.250:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","FOLD: 1\n","TRAIN: [ 0  1  3 ... 47 48 49] VALID: [ 2  4 10 11 22 27 28 31 38 41]\n","\n","EPOCH 1/20\n","time: 578.4s loss: 1.4246 accuracy: 0.5263 val_loss: 0.7530 val_accuracy: 0.8449 lr: 8e-05\n","Saved model weights at \"/content/drive/MyDrive/Colab Notebooks/Cassava/model/model_0.h5\"\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-a76009ddc960>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepoch_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __inference_train_step_758165}} Attempting to reserve 6.24G at the bottom of memory. That was not possible. There are 5.02G free, 2.00G reserved, and 6.23G reservable.; \nHBM capacity 8.00G:\n       Top-of-Memory reserved:  529.00M (     554,696,704 bytes)\n          Currently allocated:  999.09M (   1,047,625,728 bytes)\n          Program requirement:    6.24G (   6,696,993,792 bytes)\n                               ------------\n      Available after request:  277.16M (     290,618,368 bytes)\n; attempting reservation for program memory\n\nTotal hbm usage >= 7.96G:\n    reserved(Infeed/Outfeed/Continuations) 529.00M \n    heap allocations (without fragmentation) 999.09M \n    program                    6.47G \n\nHeap allocator status:\n    Max free chunk             4.22G (0.2% fragmentation)\n\nPersistent allocations include some or all of:\n    arguments                359.92M (94.5% utilization)\n    output                   308.77M (93.8% utilization) [may share some memory with arguments]\n\nProgram hbm requirement 6.47G:\n    reserved           4.0K\n    global           564.0K\n    scoped           25.68M\n    HLO temp          6.21G (96.4% utilization: Unpadded (5.95G) Padded (6.16G), 0.8% fragmentation (48.09M))\n    overlays        234.08M\n\n  Largest program allocations in hbm:\n\n  1. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3446 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18222)\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3444 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18220)\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3442 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18218)\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3440 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18216)\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3436 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18212)\n     Allocation type: HLO temp\n     ==========================\n\n  6. Size: 288.00M\n     Shape: f32[16,128,128,288]{2,3,1,0}\n     Unpadded size: 288.00M\n     XLA label: %copy.3438 = f32[16,128,128,288]{2,3,1,0} copy(f32[16,128,128,288]{3,0,2,1:T(8,128)} %get-tuple-element.18214)\n     Allocation type: HLO temp\n     ==========================\n\n  7. Size: 256.00M\n     Shape: f32[16,256,256,64]{2,3,1,0}\n     Unpadded size: 256.00M\n     XLA  ... [truncated]"]}]},{"cell_type":"markdown","metadata":{"id":"NQyHtXnRCaO5"},"source":["## Model loss graph"]},{"cell_type":"code","metadata":{"id":"_7pUdch3CaO6","executionInfo":{"status":"aborted","timestamp":1613071732905,"user_tz":-540,"elapsed":729885,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["for fold, history in enumerate(history_list):\n","    print(f'\\nFOLD: {fold+1}')\n","    plot_metrics(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImheYrtKCaO7"},"source":["# Model evaluation\n","\n","Now we can evaluate the performance of the model, first, we can evaluate the usual metrics like, `accuracy`, `precision`, `recall`, and `f1-score`, `scikit-learn` provides the perfect function for this `classification_report`.\n","\n","We are evaluating the model on the `OOF` predictions, it stands for `Out Of Fold`, since we are training using `K-Fold` our model will see all the data, and the correct way to evaluate each fold is by looking at the predictions that are not from that fold.\n","\n","## OOF metrics"]},{"cell_type":"markdown","metadata":{"id":"I_QvO8rPCaO7"},"source":["#### I am still having some problems to get the real model `OOF` scores while using `TPU Pods`, so the results here and the confusion matrix are just placeholders."]},{"cell_type":"code","metadata":{"id":"q7TfhkICCaO7","executionInfo":{"status":"aborted","timestamp":1613071732907,"user_tz":-540,"elapsed":729881,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["y_true = np.concatenate(oof_labels)\n","y_true = np.argmax(y_true, axis=-1)\n","y_pred = np.concatenate(oof_pred)\n","\n","print(classification_report(y_true, y_pred, target_names=CLASSES))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIAJafCnCaO7"},"source":["# Confusion matrix\n","\n","Let's also take a look at the confusion matrix, this will give us an idea about what classes the model is mixing or having a hard time."]},{"cell_type":"code","metadata":{"id":"dw2F8Wk2CaO7","executionInfo":{"status":"aborted","timestamp":1613071732908,"user_tz":-540,"elapsed":729877,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n","cfn_matrix = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n","cfn_matrix = (cfn_matrix.T / cfn_matrix.sum(axis=1)).T\n","df_cm = pd.DataFrame(cfn_matrix, index=CLASSES, columns=CLASSES)\n","ax = sns.heatmap(df_cm, cmap='Blues', annot=True, fmt='.2f', linewidths=.5).set_title('Train', fontsize=30)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaZVX_ZhCaO7"},"source":["# Visualize predictions\n","\n","Finally, it is a good practice to always inspect some of the model's prediction by looking at the data, this can give an idea if the model is getting some predictions wrong because the data is really hard, of if it is because the model is actually bad.\n","\n","\n","### Class map\n","```\n","0: Cassava Bacterial Blight (CBB)\n","1: Cassava Brown Streak Disease (CBSD)\n","2: Cassava Green Mottle (CGM)\n","3: Cassava Mosaic Disease (CMD)\n","4: Healthy\n","```\n","\n","\n","## Train set"]},{"cell_type":"code","metadata":{"id":"ExEWWtxyCaO8","executionInfo":{"status":"aborted","timestamp":1613071732908,"user_tz":-540,"elapsed":729871,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["train_dataset = get_dataset(TRAINING_FILENAMES, ordered=True)\n","x_samp, y_samp = dataset_to_numpy_util(train_dataset, 18)\n","y_samp = np.argmax(y_samp, axis=-1)\n","\n","x_samp_1, y_samp_1 = x_samp[:9,:,:,:], y_samp[:9]\n","samp_preds_1 = model.predict(x_samp_1, batch_size=9)\n","display_9_images_with_predictions(x_samp_1, samp_preds_1, y_samp_1)\n","\n","x_samp_2, y_samp_2 = x_samp[9:,:,:,:], y_samp[9:]\n","samp_preds_2 = model.predict(x_samp_2, batch_size=9)\n","display_9_images_with_predictions(x_samp_2, samp_preds_2, y_samp_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9CeTJAYKYQd","executionInfo":{"status":"aborted","timestamp":1613071732909,"user_tz":-540,"elapsed":729867,"user":{"displayName":"鎌田康太郎","photoUrl":"","userId":"03364795393301554726"}}},"source":["\n","save_data=True# You can immediately create a kaggle dataset from your models\n","if COLAB and save_data:\n","  \n","    import json\n","\n","\n","    \n","    data = {\"title\": \"Cassava Leaf Disease\", \n","        \"id\": \"aikhmelnytskyy/CassavaLeafDisease\", \n","        \"licenses\": [\n","                     {\n","                         \"name\": \"CC0-1.0\"\n","                      }\n","                     ]}\n","    \n","\n","    # for kaggle api Connection\n","    \n","    !kaggle datasets init -p /content/drive/MyDrive/Colab Notebooks/Cassava/\n","    \n","    with open(\"/content/drive/MyDrive/Colab Notebooks/Cassava/dataset-metadata.json\", \"w\", encoding=\"utf-8\") as file:\n","        json.dump(data, file)\n","    \n","    #if new dataset\n","    !kaggle datasets create -p /content/drive/MyDrive/Models/Cassava/\n","    #If you’d like to upload a new version of an existing dataset\n","    #!kaggle datasets version -p /content/drive/MyDrive/Models/Cassava/ -m \"Your message here\""],"execution_count":null,"outputs":[]}]}